# Tiny-ImageNet CNN Classifier ğŸ”¥

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Accuracy](https://img.shields.io/badge/Top-1%20Accuracy-65%25-brightgreen)](#-performance)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

</div>

## ğŸ“‹ Table of Contents
- [ğŸ¯ Overview](#-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ† Performance](#-performance)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“Š Model Architecture](#-model-architecture)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸ’» Usage](#-usage)
- [ğŸ“ˆ Training Details](#-training-details)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“ Results](#-results)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

## ğŸ¯ Overview
A **ResNet-inspired Convolutional Neural Network** trained from scratch on Tiny-ImageNet (200 classes, 64 Ã— 64 px).  
Through heavy data augmentation and modern regularization (CutMix, MixUp, label smoothing), it reaches **65% top-1 accuracy**.

## âœ¨ Key Features

| âœ”ï¸  | Description |
|-----|-------------|
| **Custom Architecture** | Residual CNN optimized for 64 Ã— 64 images |
| **Advanced Augmentation** | Elastic transform, ColorJitter, CoarseDropout & more via Albumentations |
| **Modern Regularization** | CutMix, MixUp, label smoothing |
| **Cosine LR Schedule** | Smooth 1e-3 â†’ 1e-5 over 200 epochs |
| **Checkpointing & Resume** | Auto-save best model and resume training |
| **Lightweight** | ~2.8M parameters, fast to train |

## ğŸ† Performance

| Metric | Value |
|--------|-------|
| **Top-1 Accuracy** | **65%** |
| Parameters | ~2.8M |
| Training Epochs | 200 |
| Dataset | Tiny-ImageNet-200 |

> Typical from-scratch baselines score 50â€“60%. This model beats them by a healthy margin.

## ğŸš€ Quick Start

```bash
# Clone repository
git clone https://github.com/RFahmi99/ImageClassification.git
cd ImageClassification

# Install dependencies
pip install -r requirements.txt

# Download dataset (â‰ˆ 244 MB)
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip && mv tiny-imagenet-200 tiny-imagenet

# Train
python train.py
```

## ğŸ“Š Model Architecture

```text
Input (3Ã—64Ã—64)
  â”‚
  â”œâ”€â”€ Conv-BN-ReLU (64)
  â”œâ”€â”€ Stage 1: 2Ã— ResidualBlock (64)
  â”œâ”€â”€ Stage 2: 2Ã— ResidualBlock (128) â†“2Ã—
  â”œâ”€â”€ Stage 3: 2Ã— ResidualBlock (256) â†“2Ã—
  â”œâ”€â”€ Stage 4: 2Ã— ResidualBlock (512) â†“2Ã—
  â”œâ”€â”€ GlobalAvgPool
  â”œâ”€â”€ Dropout 0.5
  â””â”€â”€ FC â†’ 200
```

Key design choices: residual connections for deep supervision, batch norm everywhere, global average pooling to reduce overfitting.

## ğŸ› ï¸ Installation

### Requirements
- Python 3.8+
- CUDA-capable GPU (recommended)
- PyTorch 2.0+, Albumentations 1.3+, Pillow, NumPy

```bash
pip install torch torchvision albumentations pillow numpy
```

Or:

```bash
pip install -r requirements.txt
```

### Dataset Layout
```text
tiny-imagenet/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ <class_id>/images/*.JPEG
â””â”€â”€ val/
    â”œâ”€â”€ images/*.JPEG
    â””â”€â”€ val_annotations.txt
```

## ğŸ’» Usage

### Train from scratch
```bash
python train.py
```

Training automatically resumes if a checkpoint exists in `models/weights/model.pth`.

### Inference
```python
from models.cnn import CNN
import torch, albumentations as A

model = CNN(num_classes=200)
model.load_state_dict(torch.load('models/weights/model.pth')['model_state_dict'])
model.eval()

# preprocess your image to 3Ã—64Ã—64 Tensor and run
with torch.no_grad():
    logits = model(image_tensor)
    prediction = logits.argmax(1)
```

## ğŸ“ˆ Training Details

- **Augmentations**: HorizontalFlip, RandomRotate90, ElasticTransform, ColorJitter, GaussNoise, CoarseDropout, RandomBrightnessContrast, ImageNet-style normalization.  
- **Optimizer**: AdamW, LR 1e-3, weight decay 1e-4.  
- **Regularization**: CutMix / MixUp with dynamic selection, label smoothing 0.1, dropout 0.5, gradient clipping 1.0.  
- **Scheduler**: CosineAnnealingLR for 200 epochs.

## ğŸ“ Project Structure
```text
tiny-imagenet-cnn/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cnn.py
â”‚   â””â”€â”€ weights/
â”‚       â”œâ”€â”€ model.pth
â”‚       â””â”€â”€ log.txt
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ dataset.py
â”œâ”€â”€ train.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ tiny-imagenet/   # dataset here
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

| Hyperparameter | Value | Reason |
|----------------|-------|--------|
| Batch size | 128 | Fits 8GB GPU |
| LR | 1e-3 â†’ 1e-5 | Cosine decay |
| Weight decay | 1e-4 | L2 regularization |
| Label smoothing | 0.1 | Better calibration |
| Dropout | 0.5 | Combat overfitting |

### Hardware
- GPU: NVIDIA GTX 1080 (8GB) or better
- RAM: â‰¥ 16GB
- Storage: 2GB (dataset + checkpoints)

## ğŸ“ Results

Training curves (loss â†“, accuracy â†‘) show smooth convergence without over-fitting thanks to strong regularization.

| Model | Top-1 Acc | Params | Notes |
|-------|-----------|--------|-------|
| **This work** | **65%** | 2.8M | Custom CNN |
| ResNet-18 | 50â€“60% | 11.7M | From scratch |
| VGG-16 (TL) | ~49% | 138M | Transfer learning |

## ğŸ¤ Contributing

1. Fork â†’ Create branch â†’ Commit â†’ Pull request.  
2. Run `black . && flake8 .` before submitting.  
3. Open issues for bugs or ideas.

## ğŸ“„ License

Released under the **MIT License** â€” see `LICENSE` for details.

<div align="center">

**Star this repo if you found it helpful!**

Made with â¤ï¸ for the deep-learning community.

</div>